import os
import pypandoc
import jinja2
import subprocess
import uuid
import re
import asyncio
import httpx
from starlette.concurrency import run_in_threadpool
from app.config import settings

# Configure Jinja2 for LaTeX
latex_jinja_env = jinja2.Environment(
    block_start_string='\\BLOCK{',
    block_end_string='}',
    variable_start_string='\\VAR{',
    variable_end_string='}',
    comment_start_string='\\#{',
    comment_end_string='}',
    line_statement_prefix='%%',
    line_comment_prefix='%#',
    trim_blocks=True,
    autoescape=False,
    loader=jinja2.FileSystemLoader(os.path.abspath('app/templates'))
)

def clean_math_syntax(markdown_text: str) -> str:
    """
    Restores math delimiters that might have been escaped by Pandoc.
    Changes \$\$...\$\$ back to $$...$$ and \$...\$ back to $...$
    """
    # Restore block math $$...$$
    # Pandoc escapes them as \$\$ ... \$\$
    # Regex: Look for escaped dollars, capture content, restore unescaped dollars
    # Note: We need to be careful not to unescape single escaped dollars that are meant to be literal dollars, 
    # but the context implies we are targeting the specific artifact of pandoc conversion for math.
    
    # Replace \$\$ content \$\$ -> $$ content $$
    # We use a pattern that matches the escaped sequence.
    # Pattern: \\(\$)\\(\$) -> matches \$\$
    markdown_text = re.sub(r'\\(\$)\\(\$)(.*?)\\(\$)\\(\$)', r'$$\3$$', markdown_text, flags=re.DOTALL)
    
    # Replace \$ content \$ -> $ content $
    # Pattern: \\(\$) -> matches \$
    markdown_text = re.sub(r'\\(\$)(.*?)\\(\$)', r'$\2$', markdown_text, flags=re.DOTALL)
    
    return markdown_text

def remove_emojis(text: str) -> str:
    # Remove Unicode characters that pdflatex generally fails on (Emojis, Dingbats)
    # Ranges: 
    # 1F300-1F9FF (Misc Symbols and Pictographs, Emoticons, Transport, etc.)
    # 2600-26FF (Misc Symbols)
    # 2700-27BF (Dingbats - includes ✅)
    return re.sub(r'[\U00010000-\U0010ffff]|[\u2600-\u27BF]', '', text)

def remove_emojis(text: str) -> str:
    """
    Removes emojis and other special characters that pdflatex struggles with.
    """
    # Remove surrogate pairs and other symbols often used for emojis
    # ranges: 1F600-1F64F (Emoticons), 1F300-1F5FF (Misc Symbols), 1F680-1F6FF (Transport), etc.
    # Simple regex for high unicode ranges often covers most emojis.
    return re.sub(r'[^\x00-\x7F]+', '', text) # Aggressive: Remove ALL non-ASCII. 
    # Or less aggressive: Only strip known emoji ranges.
    # Given this is an academic report, ASCII only is usually safe/preferred for stability.
    # But names might have accents. 
    # Let's try to keep accents (Latin-1) but remove others.
    # But easiest fix for stability now is filter non-ascii chars that are > 255.
    
    # Better approach: Remove characters that match emoji properties. 
    # But python standard lib doesn't have easy emoji property check without `emoji` pip package.
    
    # We will just strip specific problematic characters we saw: ✅ (U+2705)
    # And maybe use a generic "remove non-basic-latin" if needed.
    # Let's stick to removing specific ranges or just U+26.. U+27.. U+1F...
    
    return re.sub(r'[\U00010000-\U0010ffff]', '', text) # Standard Emojis

def sanitize_markdown(text: str) -> str:
    """
    Cleans up "dirty" Markdown generated from Docx.
    1. Removes Table of Contents (ghost lists).
    2. Strips auto-numbering from headers (e.g. '# 1. Intro' -> '# Intro').
    3. Fixes image captions (removes filenames).
    """
    
    # 1. Remove Table of Contents
    # Strategy: Find header "Table of Contents" and remove everything until next header.
    # Note: Pandoc might generate it as '# Table of Contents' or 'Table of Contents' followed by list.
    # We'll regex for the header and the following block.
    # Case insensitive match for header
    # ==================================================================================
    # MISSION: REMOVE MANUAL TOC GENERATION
    # The following block identifies and removes any text-based Table of Contents 
    # generated by Pandoc or found in the source doc.
    # We DO NOT pass this text to LaTeX; instead we let LaTeX generate its own TOC.
    # ==================================================================================
    text = re.sub(
        r'^(#+\s*)?(Table of Contents|Contents)\s*\n.*?(?=\n#)', 
        '', 
        text, 
        flags=re.IGNORECASE | re.DOTALL | re.MULTILINE
    )
    
    # Extra safety: Remove lines that look like just numbers or "Page x" often found in dirty ToCs
    # Pattern: Line with only whitespace, dots, and digits
    text = re.sub(r'^\s*[\.\d\s]+\s*$', '', text, flags=re.MULTILINE)
    
    # 2. Strip Header Numbering
    # Pattern: #... <number>.<number>... <Text>
    # Group 1: Hashes, Group 2: Numbering, Group 3: Text
    # we want to keep Group 1 and Group 3.
    text = re.sub(r'^(#+)\s+(?:\d+\.)+\d*\s+(.*)', r'\1 \2', text, flags=re.MULTILINE)
    # Also handle single level "1. Intro"
    text = re.sub(r'^(#+)\s+\d+\.?\s+(.*)', r'\1 \2', text, flags=re.MULTILINE)
    
    # 3. Sanitize Image Captions
    # Replace ![image.png](...) with ![Figure](...) or ![](...)
    # We look for captions that look like filenames (have extension)
    def clean_caption(match):
        caption = match.group(1)
        url = match.group(2)
        # Check if caption looks like a file (has dot extension in 3-4 chars)
        if re.search(r'\.(png|jpg|jpeg|bmp|gif|tif|tiff)$', caption, re.IGNORECASE):
             return f"![Figure]({url})"
        if "Screenshot" in caption or "image" in caption:
             return f"![Figure]({url})"
        return match.group(0)

    text = re.sub(r'!\[(.*?)\]\((.*?)\)', clean_caption, text)
    
    return text



def process_docx(file_path: str, session_id: str):
    """
    Converts .docx to Markdown and extracts media.
    Returns: (markdown_content, list_of_media_files)
    """
    # Create media directory for this session
    media_dir = os.path.join(settings.WORKSPACE_DIR, session_id, "media")
    os.makedirs(media_dir, exist_ok=True)
    
    # Output file path (temp)
    output_path = os.path.join(settings.WORKSPACE_DIR, session_id, "content.md")

    # Convert using pypandoc
    # extra_args=['--extract-media=' + media_dir] doesn't always work as expected with path resolution in pypandoc 
    # depending on cwd. Better to use absolute paths if supported or ensure cwd.
    # However, pypandoc converts and returns string or writes to file. 
    # If we write to file, we can control path.
    
    # NOTE: pypandoc wrapper often makes path handling for extract-media tricky.
    # We will try passing absolute path to extract-media.
    # The extracted media links in the markdown will initially point to this folder.
    
    # We need to make sure the media_dir passed to extract-media is one where pandoc writes images.
    # Pandoc writes images to `media_dir/image1.png` etc. and links them as `media_dir/image1.png`.
    
    # To handle this cleanly, let's use a relative path for the markdown links to match what we assume,
    # or just parse the output.
    
    try:
        output = pypandoc.convert_file(
            file_path, 
            'markdown', 
            outputfile=output_path,
            extra_args=[f'--extract-media={os.path.dirname(media_dir)}'] 
            # Note: extract-media takes a directory. If we want them in .../session_id/media, 
            # and the markdown is in .../session_id/content.md, then extracting to .../session_id 
            # should ideally result in media/image.png if the docx has them. 
            # BUT pandoc behavior varies. 
            # Let's try explicit media dir.
        )
    except RuntimeError as e:
        # Fallback or error handling
        print(f"Pandoc error: {e}")
        raise e

    # Read the markdown
    with open(output_path, 'r') as f:
        markdown_content = f.read()
        
    # Clean math syntax (restore $$...$$ and $...$)
    markdown_content = clean_math_syntax(markdown_content)

    # Sanitize Markdown (Remove ToC, Numbering, Bad Captions)
    markdown_content = sanitize_markdown(markdown_content)

        
    # List extracted images
    image_files = []
    if os.path.exists(media_dir):
        for root, dirs, files in os.walk(media_dir):
            for file in files:
                image_files.append(os.path.join(root, file))
                
    return markdown_content, image_files

async def restore_remote_images(markdown_text: str, temp_dir: str) -> str:
    """
    Downloads remote images found in markdown and replaces them with local paths.
    """
    media_dir = os.path.join(temp_dir, "media")
    os.makedirs(media_dir, exist_ok=True)
    
    # Find all images: ![Alt](url)
    # Group 1: Alt text, Group 2: URL
    # Debug Markdown
    print(f"DEBUG: Markdown text (first 500 chars): {markdown_text[:500]}")
    
    # Find all images: ![Alt](url) - Relaxed regex with DOTALL for wrapped lines
    # Group 1: Alt text, Group 2: URL
    pattern = r'!\[(.*?)\]\((.*?)\)'
    matches = re.findall(pattern, markdown_text, flags=re.DOTALL)
    
    print(f"DEBUG: Found {len(matches)} potential image matches")
    
    # Filter for http urls
    # Filter for http urls
    urls = [] # List of (original_messy_url, clean_download_url)
    
    for m in matches:
        raw_url = m[1]
        # Remove common markdown URL wrappers and NEWLINES from word wrap
        clean_url = raw_url.strip('<>').strip().replace('\n', '').replace(' ', '')
        
        print(f"DEBUG: Checking URL: '{clean_url}' (Raw: '{raw_url[:50]}...')")
        
        # Check if it contains http anywhere
        download_url = None
        if "https://" in clean_url:
             start_idx = clean_url.rfind("https://")
             download_url = clean_url[start_idx:]
        elif "http://" in clean_url:
             start_idx = clean_url.rfind("http://")
             download_url = clean_url[start_idx:]
        
        if download_url and ("storage" in download_url or "firebase" in download_url):
             # Cleanup potential trailing parens if regex missed them before
             download_url = download_url.split(')')[0]
             
             print(f"DEBUG: Extracted Viewable URL: '{download_url}' from mess")
             urls.append((raw_url, download_url))
        else:
             print(f"DEBUG: Rejected URL: {clean_url}")
             
    # Deduplicate based on download url? 
    # Logic: we need unique download URLs, but we need to replace all instances.
    # Let's map download_url -> list of original_urls to replace
    
    unique_downloads = {}
    for original, download in urls:
        if download not in unique_downloads:
             unique_downloads[download] = []
        unique_downloads[download].append(original)
        
    print(f"DEBUG: Filtered Download URLs: {list(unique_downloads.keys())}")
    
    if not unique_downloads:
        print("DEBUG: No URLs to download. Returning markdown as is.")
        return markdown_text
        
    async def download_image(client, url):
        try:
            filename = os.path.basename(url.split('?')[0]) 
            if not filename or filename == "":
                filename = f"{uuid.uuid4()}.png"
            
            dest_path = os.path.join(media_dir, filename)
            
            resp = await client.get(url, follow_redirects=True)
            if resp.status_code == 200:
                with open(dest_path, "wb") as f:
                    f.write(resp.content)
                return url, dest_path
            else:
                print(f"Failed to download {url}: {resp.status_code}")
                return url, None
        except Exception as e:
            print(f"Error downloading {url}: {e}")
            return url, None

    async with httpx.AsyncClient() as client:
        tasks = [download_image(client, d_url) for d_url in unique_downloads.keys()]
        results = await asyncio.gather(*tasks)
        
    # Replace in markdown
    print(f"DEBUG: Restore images results: {len(results)}")
    
    for download_url, local_path in results:
        if local_path:
            # Get list of original strings to replace for this download URL
            originals = unique_downloads.get(download_url, [])
            
            filename = os.path.basename(local_path)
            relative_path = f"media/{filename}"
            
            for url in originals:
                if url in markdown_text:
                    print(f"DEBUG: Replacing {url[:30]}... with {relative_path}")
                    markdown_text = markdown_text.replace(url, relative_path)
                else:
                     print(f"DEBUG: URL {url[:30]}... NOT FOUND in text during replacement")
            
    return markdown_text

def remove_emojis(text: str) -> str:
    """
    Removes emojis and non-standard characters by keeping only Latin-1.
    """
    # Keep only characters in range 0x00-0xFF
    return re.sub(r'[^\x00-\xFF]+', '', text)

async def compile_pdf(markdown_text: str, metadata: dict, session_id: str, report_type: str = "default") -> str:
    """
    Compiles Markdown + Metadata -> PDF.
    Returns: Path to generated PDF.
    """
    work_dir = os.path.join(settings.WORKSPACE_DIR, session_id)
    os.makedirs(work_dir, exist_ok=True)

    # 0. Restore Images (Async)
    markdown_text = await restore_remote_images(markdown_text, work_dir)
    markdown_text = remove_emojis(markdown_text)

    # 1. Convert Markdown Body to LaTeX Body
    # We use pandoc to convert the markdown body fragment to latex
    # Run loop blocking call in threadpool
    body_latex = await run_in_threadpool(
        pypandoc.convert_text, 
        markdown_text, 
        'latex', 
        format='markdown'
    )
    
    # 2. Render Template
    # Select template based on report_type
    template_map = {
        "Micro Project": "micro_project.tex",
        "Mini Project": "mini_project.tex",
        "Mini Skill Project": "mini_project.tex", # Map to Mini Project template
        "Skilled Based Lab": "skilled_lab.tex",
        "Macro Project": "master.tex" # Default to master for now
    }
    
    # Default to master.tex if not found or if simply "Assignment" or other
    template_name = template_map.get(report_type, "master.tex")
    print(f"DEBUG: Using template '{template_name}' for report type '{report_type}'")
    
    template = latex_jinja_env.get_template(template_name)
    
    # Prepare context
    context = {
        'title': metadata.get('title', 'Report'),
        'author_name': metadata.get('author_name', 'Student'),
        'abstract': metadata.get('abstract', ''),
        'roll_no': metadata.get('roll_no', ''),
        'department': metadata.get('department', ''),
        'guide_name': metadata.get('guide_name', ''),
        'session_year': metadata.get('session_year', ''),
        'body_content': body_latex
    }
    
    rendered_tex = template.render(context)
    
    # 3. Save .tex file
    tex_file_path = os.path.join(work_dir, "report.tex")
    
    # File I/O is blocking but fast enough; strictly one could defer it, but acceptable here.
    with open(tex_file_path, 'w') as f:
        f.write(rendered_tex)
        
    # 4. Run pdflatex
    def run_latex():
        try:
            subprocess.run(
                ['pdflatex', '-interaction=nonstopmode', 'report.tex'], 
                cwd=work_dir, 
                check=False,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            # Run twice for ToC
            result = subprocess.run(
                ['pdflatex', '-interaction=nonstopmode', 'report.tex'], 
                cwd=work_dir, 
                check=False,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            
            # Check if PDF exists
            if not os.path.exists(os.path.join(work_dir, "report.pdf")):
                 print(f"PDF Compilation failed. Output: {result.stdout.decode()}")
                 raise Exception("PDF Compilation failed (No PDF generated)")
            
            if result.returncode != 0:
                 print(f"PDF Compilation Warning (Exit Code {result.returncode}): {result.stdout.decode()[:500]}...")
                 # Proceed if PDF exists

        except subprocess.CalledProcessError as e:
            print(f"PDF Compilation failed: {e.stderr.decode()}")
            raise Exception("PDF Compilation failed")

    # Actually run it
    await run_in_threadpool(run_latex)

    return os.path.join(work_dir, "report.pdf")
